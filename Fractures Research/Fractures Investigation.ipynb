{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Sun Jan 27 21:05:20 2019',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'frac1': array([[0.4125, 0.4075, 0.4   , ..., 0.3975, 0.4   , 0.4   ],\n",
       "        [0.4625, 0.4425, 0.4275, ..., 0.425 , 0.4275, 0.425 ],\n",
       "        [0.54  , 0.5125, 0.475 , ..., 0.4575, 0.4625, 0.4625],\n",
       "        ...,\n",
       "        [0.6125, 0.6125, 0.6075, ..., 0.8025, 0.7425, 0.6325],\n",
       "        [0.6125, 0.6175, 0.6125, ..., 0.55  , 0.4725, 0.415 ],\n",
       "        [0.605 , 0.6125, 0.6125, ..., 0.34  , 0.3225, 0.335 ]]),\n",
       " 'frac1_binary': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'frac2': array([[0.6875, 0.7   , 0.7225, ..., 0.615 , 0.6125, 0.61  ],\n",
       "        [0.6675, 0.6725, 0.6725, ..., 0.59  , 0.595 , 0.6125],\n",
       "        [0.665 , 0.6625, 0.6525, ..., 0.5375, 0.555 , 0.58  ],\n",
       "        ...,\n",
       "        [0.815 , 0.775 , 0.735 , ..., 0.365 , 0.3575, 0.33  ],\n",
       "        [0.7375, 0.705 , 0.68  , ..., 0.23  , 0.22  , 0.2425],\n",
       "        [0.6725, 0.64  , 0.645 , ..., 0.1375, 0.125 , 0.1975]]),\n",
       " 'frac2_binary': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'nonfrac1': array([[0.6225, 0.61  , 0.59  , ..., 0.6825, 0.6825, 0.68  ],\n",
       "        [0.6425, 0.6225, 0.5975, ..., 0.665 , 0.6575, 0.6575],\n",
       "        [0.635 , 0.6125, 0.585 , ..., 0.65  , 0.6375, 0.64  ],\n",
       "        ...,\n",
       "        [0.6925, 0.7075, 0.7125, ..., 0.6775, 0.635 , 0.61  ],\n",
       "        [0.725 , 0.74  , 0.745 , ..., 0.5525, 0.5175, 0.49  ],\n",
       "        [0.7275, 0.7425, 0.7475, ..., 0.515 , 0.5075, 0.49  ]]),\n",
       " 'nonfrac2': array([[0.5475, 0.5575, 0.56  , ..., 0.565 , 0.565 , 0.57  ],\n",
       "        [0.5575, 0.5625, 0.5675, ..., 0.5425, 0.5425, 0.53  ],\n",
       "        [0.5725, 0.5725, 0.5775, ..., 0.5075, 0.49  , 0.47  ],\n",
       "        ...,\n",
       "        [0.7175, 0.71  , 0.71  , ..., 0.5425, 0.5375, 0.5425],\n",
       "        [0.7325, 0.7325, 0.7225, ..., 0.5325, 0.54  , 0.5325],\n",
       "        [0.76  , 0.7575, 0.755 , ..., 0.5325, 0.54  , 0.54  ]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat('fracture_sample.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracture_data_1 = loadmat('fracture_sample.mat')['frac1']\n",
    "fracture_data_2 = loadmat('fracture_sample.mat')['frac2']\n",
    "fracture_data_01 = loadmat('fracture_sample.mat')['nonfrac1']\n",
    "fracture_data_02 = loadmat('fracture_sample.mat')['nonfrac2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_data = np.array(fracture_data > 0, dtype = int)\n",
    "predicted_data_1 = loadmat('fracture_sample.mat')['frac1_binary']\n",
    "predicted_data_2 = loadmat('fracture_sample.mat')['frac2_binary']\n",
    "predicted_data_01 = np.zeros(fracture_data_01.shape)\n",
    "predicted_data_02 = np.zeros(fracture_data_02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Convolutional\n",
    "fracture_data_1 = np.pad(fracture_data_1, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "predicted_data_1 = np.pad(predicted_data_1, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "fracture_data_2 = np.pad(fracture_data_2, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "predicted_data_2 = np.pad(predicted_data_2, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "fracture_data_01 = np.pad(fracture_data_01, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "predicted_data_01 = np.pad(predicted_data_01, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "fracture_data_02 = np.pad(fracture_data_02, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))\n",
    "predicted_data_02 = np.pad(predicted_data_02, ((1, 1), (1, 1)), 'constant', \n",
    "                constant_values=((0, 0), (0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracture_data_1 = np.array([fracture_data_1[(y-1):(y+2), (x-1):(x+2)] \n",
    "              for x in np.arange(1, fracture_data_1.shape[1] - 1) \n",
    "              for y in np.arange(1, fracture_data_1.shape[0] - 1)])\n",
    "\n",
    "predicted_data_1 = np.array([predicted_data_1[y,x] \n",
    "              for x in np.arange(1, predicted_data_1.shape[1] - 1) \n",
    "              for y in np.arange(1, predicted_data_1.shape[0] - 1)])\n",
    "\n",
    "fracture_data_2 = np.array([fracture_data_2[(y-1):(y+2), (x-1):(x+2)] \n",
    "              for x in np.arange(1, fracture_data_2.shape[1] - 1) \n",
    "              for y in np.arange(1, fracture_data_2.shape[0] - 1)])\n",
    "\n",
    "predicted_data_2 = np.array([predicted_data_2[y,x] \n",
    "              for x in np.arange(1, predicted_data_2.shape[1] - 1) \n",
    "              for y in np.arange(1, predicted_data_2.shape[0] - 1)])\n",
    "\n",
    "fracture_data_01 = np.array([fracture_data_01[(y-1):(y+2), (x-1):(x+2)] \n",
    "              for x in np.arange(1, fracture_data_01.shape[1] - 1) \n",
    "              for y in np.arange(1, fracture_data_01.shape[0] - 1)])\n",
    "\n",
    "predicted_data_01 = np.array([predicted_data_01[y,x] \n",
    "              for x in np.arange(1, predicted_data_01.shape[1] - 1) \n",
    "              for y in np.arange(1, predicted_data_01.shape[0] - 1)])\n",
    "\n",
    "fracture_data_02 = np.array([fracture_data_02[(y-1):(y+2), (x-1):(x+2)] \n",
    "              for x in np.arange(1, fracture_data_02.shape[1] - 1) \n",
    "              for y in np.arange(1, fracture_data_02.shape[0] - 1)])\n",
    "\n",
    "predicted_data_02 = np.array([predicted_data_02[y,x] \n",
    "              for x in np.arange(1, predicted_data_02.shape[1] - 1) \n",
    "              for y in np.arange(1, predicted_data_02.shape[0] - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58081, 3, 3), (58081,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fracture_data_02.shape, predicted_data_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(fracture_data_1.shape[0]/2)\n",
    "valid_split = int(fracture_data_1.shape[0]*3/4)\n",
    "\n",
    "fracture_train = np.concatenate((fracture_data_1[:train_split,...],\n",
    "                           fracture_data_2[:train_split,...],\n",
    "                           fracture_data_01[:train_split,...],\n",
    "                           fracture_data_02[:train_split,...]), axis = 0)\n",
    "fracture_valid = np.concatenate((fracture_data_1[train_split:valid_split,...],\n",
    "                           fracture_data_2[train_split:valid_split,...],\n",
    "                           fracture_data_01[train_split:valid_split,...],\n",
    "                           fracture_data_02[train_split:valid_split,...]), axis = 0)\n",
    "fracture_test =  np.concatenate((fracture_data_1[valid_split:,...],\n",
    "                           fracture_data_2[valid_split:,...],\n",
    "                           fracture_data_01[valid_split:,...],\n",
    "                           fracture_data_02[valid_split:,...]), axis = 0)\n",
    "\n",
    "predicted_train = np.concatenate((predicted_data_1[:train_split,...],\n",
    "                            predicted_data_2[:train_split,...],\n",
    "                            predicted_data_01[:train_split,...],\n",
    "                            predicted_data_02[:train_split,...]), axis = 0)\n",
    "predicted_valid = np.concatenate((predicted_data_1[train_split:valid_split,...],\n",
    "                            predicted_data_2[train_split:valid_split,...],\n",
    "                            predicted_data_01[train_split:valid_split,...],\n",
    "                            predicted_data_02[train_split:valid_split,...]), axis = 0)\n",
    "predicted_test =  np.concatenate((predicted_data_1[valid_split:,...],\n",
    "                            predicted_data_2[valid_split:,...],\n",
    "                            predicted_data_01[valid_split:,...],\n",
    "                            predicted_data_02[valid_split:,...]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "train_x = np.stack([fracture_train], axis = 3)\n",
    "train_y = predicted_train\n",
    "\n",
    "train_x_mean, train_x_std = np.mean(train_x), np.std(train_x)\n",
    "\n",
    "train_x = (train_x - train_x_mean) / train_x_std\n",
    "\n",
    "valid_x = np.stack([fracture_valid], axis = 3)\n",
    "valid_y = predicted_valid\n",
    "\n",
    "valid_x = (valid_x - train_x_mean) / train_x_std\n",
    "\n",
    "test_x = np.stack([fracture_test], axis = 3)\n",
    "test_y = predicted_test\n",
    "\n",
    "test_x = (test_x - train_x_mean) / train_x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_21 (Flatten)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,033\n",
      "Trainable params: 1,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 116160 samples, validate on 58080 samples\n",
      "Epoch 1/20\n",
      "116160/116160 [==============================] - 10s 85us/step - loss: 0.2921 - acc: 0.8964 - val_loss: 0.2750 - val_acc: 0.8943\n",
      "Epoch 2/20\n",
      "116160/116160 [==============================] - 9s 77us/step - loss: 0.2212 - acc: 0.9142 - val_loss: 0.2541 - val_acc: 0.9010\n",
      "Epoch 3/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.2122 - acc: 0.9170 - val_loss: 0.2407 - val_acc: 0.9051\n",
      "Epoch 4/20\n",
      "116160/116160 [==============================] - 9s 77us/step - loss: 0.2088 - acc: 0.9185 - val_loss: 0.2421 - val_acc: 0.9046\n",
      "Epoch 5/20\n",
      "116160/116160 [==============================] - 9s 77us/step - loss: 0.2071 - acc: 0.9192 - val_loss: 0.2463 - val_acc: 0.9057\n",
      "Epoch 6/20\n",
      "116160/116160 [==============================] - 9s 77us/step - loss: 0.2053 - acc: 0.9195 - val_loss: 0.2392 - val_acc: 0.9055\n",
      "Epoch 7/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.2045 - acc: 0.9194 - val_loss: 0.2389 - val_acc: 0.9045\n",
      "Epoch 8/20\n",
      "116160/116160 [==============================] - 9s 77us/step - loss: 0.2035 - acc: 0.9198 - val_loss: 0.2529 - val_acc: 0.9003\n",
      "Epoch 9/20\n",
      "116160/116160 [==============================] - 9s 77us/step - loss: 0.2025 - acc: 0.9204 - val_loss: 0.2408 - val_acc: 0.9068\n",
      "Epoch 10/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.2017 - acc: 0.9203 - val_loss: 0.2409 - val_acc: 0.9032\n",
      "Epoch 11/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.2011 - acc: 0.9207 - val_loss: 0.2504 - val_acc: 0.9039\n",
      "Epoch 12/20\n",
      "116160/116160 [==============================] - 9s 79us/step - loss: 0.2006 - acc: 0.9212 - val_loss: 0.2421 - val_acc: 0.9052\n",
      "Epoch 13/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.1997 - acc: 0.9211 - val_loss: 0.2387 - val_acc: 0.9057\n",
      "Epoch 14/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.1993 - acc: 0.9211 - val_loss: 0.2372 - val_acc: 0.9060\n",
      "Epoch 15/20\n",
      "116160/116160 [==============================] - 10s 82us/step - loss: 0.1986 - acc: 0.9220 - val_loss: 0.2755 - val_acc: 0.8901\n",
      "Epoch 16/20\n",
      "116160/116160 [==============================] - 9s 79us/step - loss: 0.1981 - acc: 0.9218 - val_loss: 0.2367 - val_acc: 0.9055\n",
      "Epoch 17/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.1977 - acc: 0.9213 - val_loss: 0.2350 - val_acc: 0.9076\n",
      "Epoch 18/20\n",
      "116160/116160 [==============================] - 9s 78us/step - loss: 0.1972 - acc: 0.9218 - val_loss: 0.2415 - val_acc: 0.9077\n",
      "Epoch 19/20\n",
      "116160/116160 [==============================] - 9s 79us/step - loss: 0.1967 - acc: 0.9217 - val_loss: 0.2421 - val_acc: 0.9045 3s - los - ETA: 2s - loss: 0.1 - ET\n",
      "Epoch 20/20\n",
      "116160/116160 [==============================] - 9s 80us/step - loss: 0.1964 - acc: 0.9217 - val_loss: 0.2315 - val_acc: 0.9075\n",
      "[0.19621446724550332, 0.9262447489842297]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 20\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_y),\n",
    "                                                 train_y)\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "#model.add(Conv2D(64, kernel_size = (3,3), data_format = 'channels_last',\n",
    "#                     input_shape = train_x.shape[1:], kernel_initializer = 'random_uniform'))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten(input_shape = train_x.shape[1:]))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=1,\n",
    "                              verbose=0, mode='auto')]\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(valid_x, valid_y))#,\n",
    "                    #callbacks = callback)#,\n",
    "                    #class_weight=class_weights)\n",
    "\n",
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predict = model.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_y_predict = [x[0] for x in test_y_predict]\n",
    "test_y_predict = np.array(test_y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4184204 ],\n",
       "       [0.52881974],\n",
       "       [0.61232233],\n",
       "       [0.71274996],\n",
       "       [0.8529624 ],\n",
       "       [0.7998978 ],\n",
       "       [0.6232609 ],\n",
       "       [0.36000293],\n",
       "       [0.27011368],\n",
       "       [0.11161608]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, log_loss, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51663,   926],\n",
       "       [ 3358,  2137]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, (test_y_predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4994157513437719"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, (test_y_predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flat = train_x.reshape(train_x.shape[0], 9)\n",
    "valid_x_flat = valid_x.reshape(valid_x.shape[0], 9)\n",
    "test_x_flat = test_x.reshape(test_x.shape[0], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "optimal_c = 0.001\n",
    "loss = 1000\n",
    "\n",
    "for sub_c in C:\n",
    "    model = LogisticRegression(C = sub_c).fit(train_x_flat, train_y)\n",
    "    y_predict = model.predict_proba(valid_x_flat)\n",
    "    sub_loss = log_loss(valid_y, y_predict)\n",
    "    if sub_loss < loss:\n",
    "        loss = sub_loss\n",
    "        optimal_c = sub_c\n",
    "\n",
    "model_final = LogisticRegression(C = optimal_c).fit(np.concatenate((train_x_flat, valid_x_flat), axis = 0),\n",
    "                                                    np.concatenate((train_y, valid_y), axis = 0))\n",
    "y_predict = model_final.predict_proba(test_x_flat)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 0.31957894681666904)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_c, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52308,   281],\n",
       "       [ 4410,  1085]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, (y_predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3162804255939367"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, (y_predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
